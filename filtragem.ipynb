{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c6c4ea",
   "metadata": {},
   "source": [
    "### CARREGAR DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5dd5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d844e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reddit data\n",
    "\n",
    "comments = pd.read_csv('raw/comments_all.csv')\n",
    "\n",
    "submissions = pd.read_csv('raw/submissions_all.csv')\n",
    "\n",
    "print('Comments shape:', comments.shape)\n",
    "print('Submissions shape:', submissions.shape)\n",
    "\n",
    "print ('Comments columns:', comments.columns.tolist())\n",
    "print ('Submissions columns:', submissions.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[['body','id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5839476",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[['selftext', 'id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d61151",
   "metadata": {},
   "source": [
    "### FILTRAGEM INICIAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86673289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords_pt = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "\n",
    "    text = text.lower() # lowercase\n",
    "\n",
    "    text = unidecode(text) # remove accents\n",
    "\n",
    "    text = re.sub(r'[^0-9A-Za-z\\s]', '', text) # remove special characters\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "\n",
    "    tokenized_text = text.split() # split into words\n",
    "\n",
    "    tokenized_text = [word for word in tokenized_text if word not in stopwords_pt]   # remove stopwords\n",
    "    \n",
    "    tokenized_text = [word for word in tokenized_text if len(word) > 2] # remove short words\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "print(tokenize(\"OLÀ, tudo bem? \\n\\nEu sou um texto com caracteres unicode: ñ, é, ç, ü, jão, etc.\"))\n",
    "\n",
    "# convert 'body' and 'selftext' columns to string type to avoid errors during tokenization\n",
    "comments['body'] = comments['body'].astype(str)\n",
    "submissions['selftext'] = submissions['selftext'].astype(str)\n",
    "\n",
    "# tokenize the 'body' column in comments and 'selftext' column in submissions\n",
    "comments['tokenized_body'] = comments['body'].apply(tokenize)\n",
    "submissions['tokenized_selftext'] = submissions['selftext'].apply(tokenize)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter comments that contain at least two of the seed words in their tokenized body\n",
    "def contains_seed_words(tokenized_text, seed_words, min_count=2):\n",
    "    count = sum(1 for word in tokenized_text if word in seed_words)\n",
    "    return count >= min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_words = [\n",
    "    'guerra', 'guerras','ucrania', 'ucraniano', 'ucraniana', 'ucranianos', 'ucranianas','russia', 'russa', 'russo', 'russos', 'russas',\n",
    "    'conflito', 'conflitos','putin','zelensky','invasao', 'invasoes','tropa', 'tropas','nato', 'otan','sancao',\n",
    "    'sancoes', 'embargo', 'embargos','bombardeio', 'bombardeios','ocupacao', 'ocupacoes','resistencia','ofensiva', 'ofensivas','defesa', 'defesas',\n",
    "    'tanque', 'tanques', 'blindado', 'blindados','cessar-fogo', 'cessar_fogo','diplomacia','negociacao', 'negociacoes',\n",
    "    'dialogo','paz','refugiado', 'refugiada', 'refugiados', 'refugiadas','crimeia', 'crimea','donetsk',\n",
    "    'luhansk','mariupol','kiev', 'kyiv','ue', 'uniao europeia','europeu', 'europeia', 'europeus', 'europeias'\n",
    "]\n",
    "# seed words to filter comments\n",
    "\n",
    "\n",
    "comments['contains_seed_words'] = comments['tokenized_body'].apply(lambda x: contains_seed_words(x, seed_words))\n",
    "submissions['contains_seed_words'] = submissions['tokenized_selftext'].apply(lambda x: contains_seed_words(x, seed_words))\n",
    "\n",
    "# filter submissions based on the presence of seed words\n",
    "filtered_submissions = submissions[submissions['contains_seed_words']]\n",
    "# filter comments based on the presence of seed words\n",
    "filtered_comments = comments[comments['contains_seed_words']]\n",
    "\n",
    "print('Filtered Submissions shape:', filtered_submissions.shape)\n",
    "print('Filterad Comments shape:', filtered_comments.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7283f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "filtered_submissions[['selftext']].iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c18bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_comments[['body']].iloc[0:2]  # display first 2 filtered comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964c3d9",
   "metadata": {},
   "source": [
    "### TOP PALAVRAS MAIS FREQUENTES NA FILTRAGEM INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tf = {}\n",
    "\n",
    "# calculate term frequency for each token in the filtered comments and submissions\n",
    "for index, row in filtered_comments.iterrows():\n",
    "    for token in row['tokenized_body']:\n",
    "        if token not in token_tf:\n",
    "            token_tf[token] = 0\n",
    "        token_tf[token] += 1\n",
    "\n",
    "for index, row in filtered_submissions.iterrows():\n",
    "    for token in row['tokenized_selftext']:\n",
    "        if token not in token_tf:\n",
    "            token_tf[token] = 0\n",
    "        token_tf[token] += 1    \n",
    "\n",
    "# sort tokens by their term frequency\n",
    "sorted_tokens = sorted(token_tf.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# top K tokens\n",
    "K = 100\n",
    "top__tokens = sorted_tokens[:K]\n",
    "\n",
    "print(top__tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = [\n",
    "    'pais','eua', 'mundo', 'contra', 'paises', 'governo', 'area','poder', 'estado', 'pro', 'povo', 'historia', 'historica', 'historico', 'populacao'\n",
    "]\n",
    "\n",
    "# merge seed words and new words\n",
    "seed_words = set(seed_words + new_words)\n",
    "\n",
    "# filter comments and submissions again based on the updated seed words\n",
    "\n",
    "comments['contains_seed_words'] = comments['tokenized_body'].apply(lambda x: contains_seed_words(x, seed_words))\n",
    "submissions['contains_seed_words'] = submissions['tokenized_selftext'].apply(lambda x: contains_seed_words(x, seed_words))\n",
    "\n",
    "final_filtered_comments = comments[comments['contains_seed_words']]\n",
    "final_filtered_submissions = submissions[submissions['contains_seed_words']]\n",
    "\n",
    "print('Filtered Comments shape after merging new words:', final_filtered_comments.shape)\n",
    "print('Filtered Submissions shape after merging new words:', final_filtered_submissions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89936f",
   "metadata": {},
   "source": [
    "### VALIDACAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_submissions[['selftext']].iloc[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f409cb9",
   "metadata": {},
   "source": [
    "### SAVE A CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f14a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
